{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfebab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pretraining on cpu...\n",
      "Train samples: 800 | Val samples: 200\n",
      "Epoch 1/500 | Train Loss: 0.375143 | Val Loss: 0.248446\n",
      "Epoch 2/500 | Train Loss: 0.181373 | Val Loss: 0.112511\n",
      "Epoch 3/500 | Train Loss: 0.067440 | Val Loss: 0.043915\n",
      "Epoch 4/500 | Train Loss: 0.035453 | Val Loss: 0.035089\n",
      "Epoch 5/500 | Train Loss: 0.029070 | Val Loss: 0.034572\n",
      "Epoch 6/500 | Train Loss: 0.025436 | Val Loss: 0.030024\n",
      "Epoch 7/500 | Train Loss: 0.022066 | Val Loss: 0.035394\n",
      "Epoch 8/500 | Train Loss: 0.019824 | Val Loss: 0.031621\n",
      "Epoch 9/500 | Train Loss: 0.018982 | Val Loss: 0.042258\n",
      "Epoch 10/500 | Train Loss: 0.016568 | Val Loss: 0.046752\n",
      "Epoch 11/500 | Train Loss: 0.014995 | Val Loss: 0.041594\n",
      "Epoch 12/500 | Train Loss: 0.013412 | Val Loss: 0.033489\n",
      "Epoch 13/500 | Train Loss: 0.012307 | Val Loss: 0.018321\n",
      "Epoch 14/500 | Train Loss: 0.011597 | Val Loss: 0.012807\n",
      "Epoch 15/500 | Train Loss: 0.011684 | Val Loss: 0.040958\n",
      "Epoch 16/500 | Train Loss: 0.011144 | Val Loss: 0.012082\n",
      "Epoch 17/500 | Train Loss: 0.010281 | Val Loss: 0.011218\n",
      "Epoch 18/500 | Train Loss: 0.009843 | Val Loss: 0.010995\n",
      "Epoch 19/500 | Train Loss: 0.009429 | Val Loss: 0.010570\n",
      "Epoch 20/500 | Train Loss: 0.008802 | Val Loss: 0.010395\n",
      "Epoch 21/500 | Train Loss: 0.008431 | Val Loss: 0.008997\n",
      "Epoch 22/500 | Train Loss: 0.008821 | Val Loss: 0.008351\n",
      "Epoch 23/500 | Train Loss: 0.008314 | Val Loss: 0.007502\n",
      "Epoch 24/500 | Train Loss: 0.008719 | Val Loss: 0.014743\n",
      "Epoch 25/500 | Train Loss: 0.007999 | Val Loss: 0.008848\n",
      "Epoch 26/500 | Train Loss: 0.007500 | Val Loss: 0.008219\n",
      "Epoch 27/500 | Train Loss: 0.007767 | Val Loss: 0.011089\n",
      "Epoch 28/500 | Train Loss: 0.007391 | Val Loss: 0.007493\n",
      "Epoch 29/500 | Train Loss: 0.007238 | Val Loss: 0.007697\n",
      "Epoch 30/500 | Train Loss: 0.006491 | Val Loss: 0.007281\n",
      "Epoch 31/500 | Train Loss: 0.005330 | Val Loss: 0.005662\n",
      "Epoch 32/500 | Train Loss: 0.004475 | Val Loss: 0.005995\n",
      "Epoch 33/500 | Train Loss: 0.003797 | Val Loss: 0.004228\n",
      "Epoch 34/500 | Train Loss: 0.003562 | Val Loss: 0.004345\n",
      "Epoch 35/500 | Train Loss: 0.003701 | Val Loss: 0.003922\n",
      "Epoch 36/500 | Train Loss: 0.004052 | Val Loss: 0.003894\n",
      "Epoch 37/500 | Train Loss: 0.003327 | Val Loss: 0.003390\n",
      "Epoch 38/500 | Train Loss: 0.003185 | Val Loss: 0.003552\n",
      "Epoch 39/500 | Train Loss: 0.003255 | Val Loss: 0.003448\n",
      "Epoch 40/500 | Train Loss: 0.003017 | Val Loss: 0.003648\n",
      "Epoch 41/500 | Train Loss: 0.003098 | Val Loss: 0.003564\n",
      "Epoch 42/500 | Train Loss: 0.002992 | Val Loss: 0.003252\n",
      "Epoch 43/500 | Train Loss: 0.002998 | Val Loss: 0.003236\n",
      "Epoch 44/500 | Train Loss: 0.002941 | Val Loss: 0.003317\n",
      "Epoch 45/500 | Train Loss: 0.002810 | Val Loss: 0.003537\n",
      "Epoch 46/500 | Train Loss: 0.002856 | Val Loss: 0.003054\n",
      "Epoch 47/500 | Train Loss: 0.002891 | Val Loss: 0.002868\n",
      "Epoch 48/500 | Train Loss: 0.002716 | Val Loss: 0.003108\n",
      "Epoch 49/500 | Train Loss: 0.002757 | Val Loss: 0.003064\n",
      "Epoch 50/500 | Train Loss: 0.002803 | Val Loss: 0.003222\n",
      "Epoch 51/500 | Train Loss: 0.002624 | Val Loss: 0.002498\n",
      "Epoch 52/500 | Train Loss: 0.002612 | Val Loss: 0.002812\n",
      "Epoch 53/500 | Train Loss: 0.002455 | Val Loss: 0.002835\n",
      "Epoch 54/500 | Train Loss: 0.002455 | Val Loss: 0.002591\n",
      "Epoch 55/500 | Train Loss: 0.002412 | Val Loss: 0.002490\n",
      "Epoch 56/500 | Train Loss: 0.002316 | Val Loss: 0.002384\n",
      "Epoch 57/500 | Train Loss: 0.002371 | Val Loss: 0.002835\n",
      "Epoch 58/500 | Train Loss: 0.002566 | Val Loss: 0.002308\n",
      "Epoch 59/500 | Train Loss: 0.003203 | Val Loss: 0.004497\n",
      "Epoch 60/500 | Train Loss: 0.005634 | Val Loss: 0.042821\n",
      "Epoch 61/500 | Train Loss: 0.003441 | Val Loss: 0.002666\n",
      "Epoch 62/500 | Train Loss: 0.002194 | Val Loss: 0.002239\n",
      "Epoch 63/500 | Train Loss: 0.002003 | Val Loss: 0.002114\n",
      "Epoch 64/500 | Train Loss: 0.002016 | Val Loss: 0.002060\n",
      "Epoch 65/500 | Train Loss: 0.001952 | Val Loss: 0.002063\n",
      "Epoch 66/500 | Train Loss: 0.001902 | Val Loss: 0.001809\n",
      "Epoch 67/500 | Train Loss: 0.001923 | Val Loss: 0.001974\n",
      "Epoch 68/500 | Train Loss: 0.001873 | Val Loss: 0.001749\n",
      "Epoch 69/500 | Train Loss: 0.001821 | Val Loss: 0.001944\n",
      "Epoch 70/500 | Train Loss: 0.001791 | Val Loss: 0.001934\n",
      "Epoch 71/500 | Train Loss: 0.001787 | Val Loss: 0.001835\n",
      "Epoch 72/500 | Train Loss: 0.001769 | Val Loss: 0.001943\n",
      "Epoch 73/500 | Train Loss: 0.001690 | Val Loss: 0.001778\n",
      "Epoch 74/500 | Train Loss: 0.001750 | Val Loss: 0.001819\n",
      "Epoch 75/500 | Train Loss: 0.001695 | Val Loss: 0.001729\n",
      "Epoch 76/500 | Train Loss: 0.001639 | Val Loss: 0.001727\n",
      "Epoch 77/500 | Train Loss: 0.001658 | Val Loss: 0.001735\n",
      "Epoch 78/500 | Train Loss: 0.001613 | Val Loss: 0.001842\n",
      "Epoch 79/500 | Train Loss: 0.001678 | Val Loss: 0.001716\n",
      "Epoch 80/500 | Train Loss: 0.001670 | Val Loss: 0.001767\n",
      "Epoch 81/500 | Train Loss: 0.001623 | Val Loss: 0.001591\n",
      "Epoch 82/500 | Train Loss: 0.001590 | Val Loss: 0.001868\n",
      "Epoch 83/500 | Train Loss: 0.001632 | Val Loss: 0.001727\n",
      "Epoch 84/500 | Train Loss: 0.001598 | Val Loss: 0.001636\n",
      "Epoch 85/500 | Train Loss: 0.001569 | Val Loss: 0.001575\n",
      "Epoch 86/500 | Train Loss: 0.001586 | Val Loss: 0.001657\n",
      "Epoch 87/500 | Train Loss: 0.001577 | Val Loss: 0.001570\n",
      "Epoch 88/500 | Train Loss: 0.001517 | Val Loss: 0.001533\n",
      "Epoch 89/500 | Train Loss: 0.001520 | Val Loss: 0.001683\n",
      "Epoch 90/500 | Train Loss: 0.001482 | Val Loss: 0.001565\n",
      "Epoch 91/500 | Train Loss: 0.001502 | Val Loss: 0.001624\n",
      "Epoch 92/500 | Train Loss: 0.001529 | Val Loss: 0.001615\n",
      "Epoch 93/500 | Train Loss: 0.001502 | Val Loss: 0.001655\n",
      "Epoch 94/500 | Train Loss: 0.001535 | Val Loss: 0.001503\n",
      "Epoch 95/500 | Train Loss: 0.001475 | Val Loss: 0.001564\n",
      "Epoch 96/500 | Train Loss: 0.001468 | Val Loss: 0.001617\n",
      "Epoch 97/500 | Train Loss: 0.001461 | Val Loss: 0.001497\n",
      "Epoch 98/500 | Train Loss: 0.001498 | Val Loss: 0.001509\n",
      "Epoch 99/500 | Train Loss: 0.002999 | Val Loss: 0.061508\n",
      "Epoch 100/500 | Train Loss: 0.004664 | Val Loss: 0.003211\n",
      "Epoch 101/500 | Train Loss: 0.001652 | Val Loss: 0.002573\n",
      "Epoch 102/500 | Train Loss: 0.001506 | Val Loss: 0.003449\n",
      "Epoch 103/500 | Train Loss: 0.001440 | Val Loss: 0.001853\n",
      "Epoch 104/500 | Train Loss: 0.001468 | Val Loss: 0.001725\n",
      "Epoch 105/500 | Train Loss: 0.001415 | Val Loss: 0.001755\n",
      "Epoch 106/500 | Train Loss: 0.001405 | Val Loss: 0.001608\n",
      "Epoch 107/500 | Train Loss: 0.001450 | Val Loss: 0.001578\n",
      "Epoch 108/500 | Train Loss: 0.001406 | Val Loss: 0.001635\n",
      "Epoch 109/500 | Train Loss: 0.001371 | Val Loss: 0.001617\n",
      "Epoch 110/500 | Train Loss: 0.001412 | Val Loss: 0.001649\n",
      "Epoch 111/500 | Train Loss: 0.001370 | Val Loss: 0.001770\n",
      "Epoch 112/500 | Train Loss: 0.001379 | Val Loss: 0.001530\n",
      "Creating checkpoint... Early stopping triggered at Epoch 112\n",
      "✅ Pretraining Complete. Best model saved to ..\\src\\smartcook\\pretrained_encoder.pth\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# --- 1. SETUP PATHS & IMPORTS ---\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from smartcook.utils import EarlyStopping\n",
    "# Import the new Dataset class\n",
    "from smartcook.data_gen import CookingDataset\n",
    "from smartcook.models import MaskedCookingAutoencoder\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_EPOCHS = 500\n",
    "PATIENCE = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Ensure the directory exists\n",
    "save_dir = os.path.join('..', 'src', 'smartcook')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "SAVE_PATH = os.path.join(save_dir, 'pretrained_encoder.pth')\n",
    "\n",
    "# --- 3. DATA PREPARATION ---\n",
    "# Initialize the dataset with 1000 simulated sessions\n",
    "full_dataset = CookingDataset(num_samples=1000)\n",
    "\n",
    "# Split: 80% Train, 20% Validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 4. MODEL SETUP ---\n",
    "model = MaskedCookingAutoencoder(input_dim=3).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Initialize Early Stopping\n",
    "early_stopping = EarlyStopping(patience=PATIENCE, path=SAVE_PATH)\n",
    "\n",
    "# --- 5. TRAINING LOOP ---\n",
    "print(f\"Starting Pretraining on {DEVICE}...\")\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # -- Training Phase --\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Batch is just 'x' because our dataset returns x (no targets needed for autoencoder yet)\n",
    "        inputs = batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # The model returns (reconstructed, hidden), we only need reconstructed for loss\n",
    "        outputs, _ = model(inputs, mask_ratio=0.2) \n",
    "        \n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train = train_loss / len(train_loader)\n",
    "\n",
    "    # -- Validation Phase --\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch.to(DEVICE)\n",
    "            # No masking during validation testing\n",
    "            outputs, _ = model(inputs, mask_ratio=0.0)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_val = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} | Train Loss: {avg_train:.6f} | Val Loss: {avg_val:.6f}\")\n",
    "\n",
    "    # -- Check Early Stopping --\n",
    "    early_stopping(avg_val, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Creating checkpoint... Early stopping triggered at Epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load the best model found\n",
    "model.load_state_dict(torch.load(SAVE_PATH))\n",
    "print(f\"✅ Pretraining Complete. Best model saved to {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
